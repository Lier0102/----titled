import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin, urlparse
import time
from datetime import datetime
import json

class TistoryBlogSummarizer:
    def __init__(self):
        self.base_url = "https://stackframe.tistory.com"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
        
    def get_page_content(self, url):
        """í˜ì´ì§€ ë‚´ìš©ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""
        try:
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            response.encoding = 'utf-8'
            return response.text
        except requests.RequestException as e:
            print(f"âŒ í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨: {url} - {e}")
            return None
    
    def extract_post_links(self, html_content):
        """ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ë§í¬ë“¤ì„ ì¶”ì¶œ (ì‹¤ì œ HTML êµ¬ì¡° ê¸°ë°˜)"""
        soup = BeautifulSoup(html_content, 'html.parser')
        post_links = []
        
        # ì‹¤ì œ HTMLì—ì„œ í¬ìŠ¤íŠ¸ ë§í¬ëŠ” .post-item > a êµ¬ì¡°
        post_items = soup.select('.post-item a')
        
        for link in post_items:
            href = link.get('href')
            if href:
                # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                if href.startswith('/'):
                    full_url = self.base_url + href
                else:
                    full_url = urljoin(self.base_url, href)
                
                # ìˆ«ìë¡œë§Œ ëœ ê²½ë¡œ (ì˜ˆ: /47, /46) í™•ì¸
                if re.match(r'^https://stackframe\.tistory\.com/\d+$', full_url):
                    post_links.append(full_url)
        
        return list(set(post_links))  # ì¤‘ë³µ ì œê±°
    
    def extract_post_content(self, html_content, url):
        """ê°œë³„ í¬ìŠ¤íŠ¸ì˜ ë‚´ìš©ì„ ì¶”ì¶œ (ì‹¤ì œ í‹°ìŠ¤í† ë¦¬ êµ¬ì¡° ê¸°ë°˜)"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        post_data = {
            'url': url,
            'title': '',
            'content': '',
            'date': '',
            'tags': [],
            'category': '',
            'excerpt': ''
        }
        
        # ì œëª© ì¶”ì¶œ - í‹°ìŠ¤í† ë¦¬ íŠ¹í™”
        title_selectors = [
            '.titleWrap',
            '.entry-title', 
            'h1.title',
            '.post-title',
            'h1',
            '.article-title'
        ]
        
        for selector in title_selectors:
            title_elem = soup.select_one(selector)
            if title_elem:
                post_data['title'] = title_elem.get_text().strip()
                break
        
        # ë‚´ìš© ì¶”ì¶œ - í‹°ìŠ¤í† ë¦¬ ì—ë””í„° êµ¬ì¡°
        content_selectors = [
            '.entry-content',
            '.contents_style',
            '.post-content',
            '.article-view',
            '#content .inner',
            '.post-body'
        ]
        
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
                for unwanted in content_elem(["script", "style", "iframe", "embed", "noscript", ".another_category"]):
                    unwanted.decompose()
                
                # ê´‘ê³ ë‚˜ ê¸°íƒ€ ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                for ad in content_elem.select('.revenue_unit_item, .adsbygoogle, [id*="google_ads"]'):
                    ad.decompose()
                
                post_data['content'] = content_elem.get_text().strip()
                break
        
        # ë‚ ì§œ ì¶”ì¶œ - í‹°ìŠ¤í† ë¦¬ ë©”íƒ€ë°ì´í„°ì—ì„œ
        date_selectors = [
            '.article-date',
            '.post-date',
            '.date',
            'time[datetime]',
            '.published'
        ]
        
        for selector in date_selectors:
            date_elem = soup.select_one(selector)
            if date_elem:
                # datetime ì†ì„±ì´ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
                datetime_attr = date_elem.get('datetime')
                if datetime_attr:
                    post_data['date'] = datetime_attr
                else:
                    post_data['date'] = date_elem.get_text().strip()
                break
        
        # íƒœê·¸ ì¶”ì¶œ - í‹°ìŠ¤í† ë¦¬ íƒœê·¸ êµ¬ì¡°
        tag_selectors = [
            '.tag-list a',
            '.post-tag a',
            '.entry-tag a',
            '.tags a'
        ]
        
        for selector in tag_selectors:
            tag_elems = soup.select(selector)
            if tag_elems:
                post_data['tags'] = [tag.get_text().strip().replace('#', '') for tag in tag_elems]
                break
        
        # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ - í‹°ìŠ¤í† ë¦¬ ì¹´í…Œê³ ë¦¬
        category_selectors = [
            '.category a',
            '.post-category',
            '.entry-category',
            '.breadcrumb a:last-child'
        ]
        
        for selector in category_selectors:
            category_elem = soup.select_one(selector)
            if category_elem:
                post_data['category'] = category_elem.get_text().strip()
                break
        
        return post_data
    
    def summarize_content(self, content, max_sentences=3):
        """ê¸°ìˆ  ë¸”ë¡œê·¸ì— íŠ¹í™”ëœ ë‚´ìš© ìš”ì•½"""
        if not content:
            return ""
        
        # ë¶ˆí•„ìš”í•œ ê³µë°±ê³¼ ì¤„ë°”ê¿ˆ ì •ë¦¬
        content = re.sub(r'\s+', ' ', content).strip()
        
        # ë¬¸ì¥ ë¶„ë¦¬ (í•œêµ­ì–´ íŠ¹í™”)
        sentences = re.split(r'[.!?]\s+|\.(?=\s[A-Zê°€-í£])', content)
        sentences = [s.strip() for s in sentences if len(s.strip()) > 15]
        
        if len(sentences) <= max_sentences:
            return ' '.join(sentences[:max_sentences])
        
        # ê¸°ìˆ  ë¸”ë¡œê·¸ íŠ¹í™” í‚¤ì›Œë“œ
        tech_keywords = [
            # ì‹œìŠ¤í…œ/ì¸í”„ë¼
            'systemd', 'linux', 'ubuntu', 'centos', 'docker', 'kubernetes', 'aws', 'gcp', 'azure',
            # ë³´ì•ˆ
            'fido', 'webauthn', 'ì¸ì¦', 'ë³´ì•ˆ', 'ì•”í˜¸í™”', '2ë‹¨ê³„', 'ssl', 'tls',
            # ë°ì´í„°ë² ì´ìŠ¤
            'postgresql', 'mysql', 'mongodb', 'redis', 'database', 'schema', 'sql',
            # ë„¤íŠ¸ì›Œí¬
            'network', 'ë„¤íŠ¸ì›Œí¬', 'bandwidth', 'ëŒ€ì—­í­', 'iperf', 'vpn',
            # ê°œë°œ ì¼ë°˜
            'ê°œë°œ', 'í”„ë¡œê·¸ë˜ë°', 'ì½”ë”©', 'êµ¬í˜„', 'ì„¤ì •', 'ì„¤ì¹˜', 'ì‚¬ìš©ë²•', 'ë°©ë²•'
        ]
        
        sentence_scores = []
        for sentence in sentences:
            score = 0
            sentence_lower = sentence.lower()
            
            # ê¸°ìˆ  í‚¤ì›Œë“œ ì ìˆ˜
            for keyword in tech_keywords:
                if keyword in sentence_lower:
                    score += 2
            
            # ë¬¸ì¥ ìœ„ì¹˜ ì ìˆ˜ (ì•ìª½ ë¬¸ì¥ì— ê°€ì‚°ì )
            position_score = max(0, 3 - sentences.index(sentence) // 3)
            score += position_score
            
            # ë¬¸ì¥ ê¸¸ì´ ì ìˆ˜ (ì ì ˆí•œ ê¸¸ì´)
            if 30 <= len(sentence) <= 150:
                score += 1
            elif 150 < len(sentence) <= 250:
                score += 0.5
            
            # ìˆ«ìë‚˜ êµ¬ì²´ì  ì •ë³´ê°€ ìˆëŠ” ë¬¸ì¥ ê°€ì‚°ì 
            if re.search(r'\d+', sentence):
                score += 0.5
            
            # ëª…ë ¹ì–´ë‚˜ ì½”ë“œê°€ í¬í•¨ëœ ë¬¸ì¥
            if re.search(r'[a-zA-Z]+-[a-zA-Z]+|[a-zA-Z]+\.[a-zA-Z]+', sentence):
                score += 1
            
            sentence_scores.append((sentence, score))
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ ë¬¸ì¥ë“¤ ì„ íƒ
        sentence_scores.sort(key=lambda x: x[1], reverse=True)
        
        # ìƒìœ„ ë¬¸ì¥ë“¤ì„ ì›ë˜ ìˆœì„œëŒ€ë¡œ ì¬ì •ë ¬
        top_sentences_with_index = []
        for sentence, score in sentence_scores[:max_sentences * 2]:  # ì—¬ìœ ìˆê²Œ ì„ íƒ
            original_index = sentences.index(sentence)
            top_sentences_with_index.append((original_index, sentence, score))
        
        # ì›ë˜ ìˆœì„œëŒ€ë¡œ ì •ë ¬í•˜ê³  ìµœì¢… ì„ íƒ
        top_sentences_with_index.sort(key=lambda x: x[0])
        final_sentences = [item[1] for item in top_sentences_with_index[:max_sentences]]
        
        return ' '.join(final_sentences)
    
    def get_blog_info(self, html_content):
        """ë¸”ë¡œê·¸ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        blog_info = {
            'title': '',
            'description': '',
            'total_posts': 0,
            'categories': []
        }
        
        # ë¸”ë¡œê·¸ ì œëª©
        title_elem = soup.select_one('h1 a, .logo a, header h1')
        if title_elem:
            blog_info['title'] = title_elem.get_text().strip()
        
        # ì´ í¬ìŠ¤íŠ¸ ìˆ˜ (ì „ì²´ ê¸€ ì˜† ìˆ«ìì—ì„œ)
        total_elem = soup.select_one('.post-header span:contains("(")')
        if total_elem:
            total_text = total_elem.get_text()
            numbers = re.findall(r'\((\d+)\)', total_text)
            if numbers:
                blog_info['total_posts'] = int(numbers[0])
        
        # ì¹´í…Œê³ ë¦¬ ì •ë³´
        category_links = soup.select('.tt_category .link_item')
        for cat_link in category_links:
            cat_name = cat_link.get_text().strip()
            # ì¹´í…Œê³ ë¦¬ëª…ì—ì„œ ìˆ«ì ë¶€ë¶„ ì œê±°
            cat_name_clean = re.sub(r'\s*\(\d+\)\s*$', '', cat_name)
            if cat_name_clean:
                blog_info['categories'].append(cat_name_clean)
        
        return blog_info
    
    def analyze_blog(self, max_posts=15):
        """ë¸”ë¡œê·¸ ì „ì²´ ë¶„ì„ (ì‹¤ì œ stackframe.tistory.com êµ¬ì¡° ê¸°ë°˜)"""
        print(f"ğŸ” {self.base_url} ë¸”ë¡œê·¸ ë¶„ì„ ì‹œì‘...")
        
        # ë©”ì¸ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
        main_content = self.get_page_content(self.base_url)
        if not main_content:
            print("âŒ ë©”ì¸ í˜ì´ì§€ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # ë¸”ë¡œê·¸ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
        blog_info = self.get_blog_info(main_content)
        print(f"ğŸ“š ë¸”ë¡œê·¸: {blog_info['title']}")
        print(f"ğŸ“Š ì´ í¬ìŠ¤íŠ¸ ìˆ˜: {blog_info['total_posts']}ê°œ")
        print(f"ğŸ“ ì¹´í…Œê³ ë¦¬: {', '.join(blog_info['categories'])}")
        
        # í¬ìŠ¤íŠ¸ ë§í¬ ì¶”ì¶œ
        post_links = self.extract_post_links(main_content)
        print(f"ğŸ“ ë©”ì¸ í˜ì´ì§€ì—ì„œ {len(post_links)}ê°œì˜ í¬ìŠ¤íŠ¸ ë§í¬ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
        
        # ì¶”ê°€ í˜ì´ì§€ì—ì„œë„ í¬ìŠ¤íŠ¸ ìˆ˜ì§‘ (í˜ì´ì§€ë„¤ì´ì…˜)
        all_post_links = post_links.copy()
        
        # 2í˜ì´ì§€, 3í˜ì´ì§€ë„ í™•ì¸
        for page in range(2, 4):
            page_url = f"{self.base_url}/?page={page}"
            print(f"ğŸ“„ {page}í˜ì´ì§€ í™•ì¸ ì¤‘...")
            page_content = self.get_page_content(page_url)
            if page_content:
                page_links = self.extract_post_links(page_content)
                new_links = [link for link in page_links if link not in all_post_links]
                all_post_links.extend(new_links)
                print(f"   â”” {len(new_links)}ê°œ ì¶”ê°€ ë°œê²¬")
                time.sleep(1)  # í˜ì´ì§€ ê°„ ë”œë ˆì´
        
        print(f"ğŸ“ ì´ {len(all_post_links)}ê°œì˜ í¬ìŠ¤íŠ¸ ë§í¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
        
        if not all_post_links:
            print("âŒ í¬ìŠ¤íŠ¸ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # ìµœëŒ€ ê°œìˆ˜ë§Œí¼ë§Œ ì²˜ë¦¬
        selected_links = all_post_links[:max_posts]
        
        blog_summary = {
            'blog_url': self.base_url,
            'blog_info': blog_info,
            'analyzed_at': datetime.now().isoformat(),
            'total_posts_found': len(all_post_links),
            'analyzed_posts': len(selected_links),
            'posts': []
        }
        
        # ê° í¬ìŠ¤íŠ¸ ë¶„ì„
        for i, post_url in enumerate(selected_links, 1):
            print(f"ğŸ“– í¬ìŠ¤íŠ¸ {i}/{len(selected_links)} ë¶„ì„ ì¤‘: {post_url}")
            
            post_content = self.get_page_content(post_url)
            if not post_content:
                print(f"   âŒ í¬ìŠ¤íŠ¸ ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                continue
            
            post_data = self.extract_post_content(post_content, post_url)
            
            # ì œëª©ì´ ì—†ìœ¼ë©´ URLì—ì„œ ì¶”ì¶œ ì‹œë„
            if not post_data['title']:
                post_id = post_url.split('/')[-1]
                post_data['title'] = f"í¬ìŠ¤íŠ¸ #{post_id}"
            
            # ë‚´ìš© ìš”ì•½
            if post_data['content']:
                post_data['summary'] = self.summarize_content(post_data['content'])
                post_data['word_count'] = len(post_data['content'])
                print(f"   âœ… '{post_data['title'][:50]}...' ({post_data['word_count']}ì)")
            else:
                post_data['summary'] = "ë‚´ìš©ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                post_data['word_count'] = 0
                print(f"   âš ï¸ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨")
            
            blog_summary['posts'].append(post_data)
            
            # ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ë”œë ˆì´
            time.sleep(2)
        
        return blog_summary
    
    def generate_report(self, blog_data):
        """ìƒì„¸í•œ ë¶„ì„ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±"""
        if not blog_data or not blog_data['posts']:
            return "âŒ ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        report = []
        report.append("=" * 80)
        report.append(f"ğŸ“Š {blog_data['blog_info']['title']} ë¸”ë¡œê·¸ ë¶„ì„ ë¦¬í¬íŠ¸")
        report.append("=" * 80)
        report.append(f"ï¿½ UR L: {blog_data['blog_url']}")
        report.append(f"ï¿½ ë¶„ì„ëœ ì‹œê°„: {blog_data['analyzed_at'][:19].replace('T', ' ')}")
        report.append(f"ğŸ“š ì „ì²´ í¬ìŠ¤íŠ¸ ìˆ˜: {blog_data['blog_info']['total_posts']}ê°œ")
        report.append(f"ğŸ“ ë¶„ì„ëœ í¬ìŠ¤íŠ¸ ìˆ˜: {blog_data['analyzed_posts']}ê°œ")
        report.append(f"ğŸ“ ì¹´í…Œê³ ë¦¬: {', '.join(blog_data['blog_info']['categories'])}")
        report.append("")
        
        # í†µê³„ ê³„ì‚°
        total_words = sum(post['word_count'] for post in blog_data['posts'])
        valid_posts = [post for post in blog_data['posts'] if post['word_count'] > 0]
        
        if valid_posts:
            avg_words = total_words // len(valid_posts)
            longest_post = max(valid_posts, key=lambda x: x['word_count'])
            shortest_post = min(valid_posts, key=lambda x: x['word_count'])
            
            report.append("ğŸ“Š í†µê³„ ì •ë³´:")
            report.append(f"   â€¢ ì´ ë‹¨ì–´ ìˆ˜: {total_words:,}ê°œ")
            report.append(f"   â€¢ í‰ê·  í¬ìŠ¤íŠ¸ ê¸¸ì´: {avg_words:,}ê°œ ë‹¨ì–´")
            report.append(f"   â€¢ ê°€ì¥ ê¸´ í¬ìŠ¤íŠ¸: {longest_post['word_count']:,}ê°œ ë‹¨ì–´")
            report.append(f"   â€¢ ê°€ì¥ ì§§ì€ í¬ìŠ¤íŠ¸: {shortest_post['word_count']:,}ê°œ ë‹¨ì–´")
            report.append("")
        
        # ì£¼ìš” í‚¤ì›Œë“œ ë¶„ì„
        all_content = ' '.join([post['content'] for post in blog_data['posts']])
        tech_keywords = ['systemd', 'linux', 'fido', 'postgresql', 'google', 'security', 'network']
        found_keywords = []
        for keyword in tech_keywords:
            count = all_content.lower().count(keyword.lower())
            if count > 0:
                found_keywords.append(f"{keyword}({count})")
        
        if found_keywords:
            report.append(f"ğŸ” ì£¼ìš” ê¸°ìˆ  í‚¤ì›Œë“œ: {', '.join(found_keywords[:10])}")
            report.append("")
        
        # í¬ìŠ¤íŠ¸ë³„ ìƒì„¸ ì •ë³´
        report.append("ğŸ“‹ í¬ìŠ¤íŠ¸ ëª©ë¡:")
        report.append("=" * 80)
        
        for i, post in enumerate(blog_data['posts'], 1):
            report.append(f"\nğŸ“„ {i}. {post['title']}")
            report.append(f"ğŸ”— {post['url']}")
            
            if post['date']:
                report.append(f"ğŸ“… ì‘ì„±ì¼: {post['date']}")
            
            if post['category']:
                report.append(f"ğŸ“ ì¹´í…Œê³ ë¦¬: {post['category']}")
            
            if post['tags']:
                report.append(f"ğŸ·ï¸ íƒœê·¸: {', '.join(post['tags'])}")
            
            report.append(f"ğŸ“Š ê¸¸ì´: {post['word_count']:,}ì")
            
            if post['summary']:
                # ìš”ì•½ì„ ì ì ˆí•œ ê¸¸ì´ë¡œ ìë¥´ê¸°
                summary = post['summary']
                if len(summary) > 200:
                    summary = summary[:200] + "..."
                report.append(f"ğŸ“ ìš”ì•½: {summary}")
            
            report.append("-" * 60)
        
        return "\n".join(report)
    
    def save_results(self, blog_data, filename=None):
        """ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"blog_analysis_{timestamp}.json"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(blog_data, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return filename
        except Exception as e:
            print(f"âŒ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            return None

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ stackframe.tistory.com ë¸”ë¡œê·¸ ë¶„ì„ê¸° ì‹œì‘!")
    print("=" * 60)
    
    # ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = TistoryBlogSummarizer()
    
    # ë¸”ë¡œê·¸ ë¶„ì„ (ìµœëŒ€ 20ê°œ í¬ìŠ¤íŠ¸)
    blog_data = analyzer.analyze_blog(max_posts=20)
    
    if not blog_data:
        print("âŒ ë¸”ë¡œê·¸ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return
    
    print("\n" + "=" * 60)
    print("ğŸ“Š ë¶„ì„ ì™„ë£Œ! ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
    
    # ë¦¬í¬íŠ¸ ìƒì„± ë° ì¶œë ¥
    report = analyzer.generate_report(blog_data)
    print("\n" + report)
    
    # ê²°ê³¼ ì €ì¥
    saved_file = analyzer.save_results(blog_data)
    
    # í•µì‹¬ ì¸ì‚¬ì´íŠ¸
    if blog_data['posts']:
        print("\n" + "=" * 60)
        print("ğŸ¯ í•µì‹¬ ì¸ì‚¬ì´íŠ¸:")
        
        valid_posts = [p for p in blog_data['posts'] if p['word_count'] > 0]
        
        if valid_posts:
            # ê°€ì¥ ê¸´ í¬ìŠ¤íŠ¸
            longest_post = max(valid_posts, key=lambda x: x['word_count'])
            print(f"ï¿½ ìµœê°€ì¥ ìƒì„¸í•œ í¬ìŠ¤íŠ¸: '{longest_post['title'][:40]}...' ({longest_post['word_count']:,}ì)")
            
            # ì£¼ìš” ì£¼ì œ ë¶„ì„
            all_titles = ' '.join([p['title'] for p in blog_data['posts']])
            tech_topics = ['systemd', 'FIDO', 'PostgreSQL', 'Google', 'Solo', 'Linux', 'ë³´ì•ˆ', 'ì¸ì¦']
            found_topics = []
            for topic in tech_topics:
                if topic.lower() in all_titles.lower():
                    found_topics.append(topic)
            
            if found_topics:
                print(f"ğŸ” ì£¼ìš” ë‹¤ë£¨ëŠ” ì£¼ì œ: {', '.join(found_topics[:5])}")
            
            # ì¹´í…Œê³ ë¦¬ ë¶„ì„
            categories = {}
            for post in blog_data['posts']:
                cat = post['category'] or 'ë¯¸ë¶„ë¥˜'
                categories[cat] = categories.get(cat, 0) + 1
            
            if categories:
                top_categories = sorted(categories.items(), key=lambda x: x[1], reverse=True)[:3]
                print(f"ğŸ“ ì£¼ìš” ì¹´í…Œê³ ë¦¬: {', '.join([f'{cat}({count})' for cat, count in top_categories])}")
        
        # ë¸”ë¡œê·¸ íŠ¹ì„± ë¶„ì„
        print(f"\nğŸ’¡ ë¸”ë¡œê·¸ íŠ¹ì„±:")
        print(f"   â€¢ ê¸°ìˆ  ë¸”ë¡œê·¸ë¡œ Linux, ë³´ì•ˆ, ë°ì´í„°ë² ì´ìŠ¤ ë“±ì„ ë‹¤ë£¸")
        print(f"   â€¢ ì‹¤ë¬´ ì¤‘ì‹¬ì˜ ìƒì„¸í•œ ê°€ì´ë“œì™€ ê²½í—˜ ê³µìœ ")
        print(f"   â€¢ systemd, FIDO ì¸ì¦, PostgreSQL ë“± ì „ë¬¸ ê¸°ìˆ  ì˜ì—­ì— íŠ¹í™”")
        
        if saved_file:
            print(f"\nğŸ’¾ ìƒì„¸ ë¶„ì„ ê²°ê³¼ê°€ '{saved_file}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    print("\nâœ… stackframe.tistory.com ë¸”ë¡œê·¸ ë¶„ì„ ì™„ë£Œ!")
    print("ğŸ” ì´ ë¸”ë¡œê·¸ëŠ” ì‹œìŠ¤í…œ ê´€ë¦¬, ë³´ì•ˆ, ë°ì´í„°ë² ì´ìŠ¤ ë“± ì¸í”„ë¼ ê¸°ìˆ ì— íŠ¹í™”ëœ ì „ë¬¸ ê¸°ìˆ  ë¸”ë¡œê·¸ì…ë‹ˆë‹¤.")

if __name__ == "__main__":
    main()